# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pH-7Gi7tyAXIJEJEx4Lu_nECsY-iJhws
"""

import pandas as pd
import matplotlib.pyplot as plt
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
from tqdm import tqdm
from wordcloud import WordCloud
import seaborn as sns

import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import NMF
from collections import defaultdict

df = pd.read_csv('abcnews-date-text.csv')
df['publish_date'] = pd.to_datetime(df['publish_date'], format='%Y%m%d')
df['year'] = df['publish_date'].dt.year

# Load the tokenizer and model
model_name = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Batch processing function
def process_batches(texts, batch_size=32):
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        inputs = tokenizer(batch, padding=True, truncation=True, return_tensors="pt", max_length=128).to(device)
        with torch.no_grad():
            outputs = model(**inputs)
        yield torch.nn.functional.softmax(outputs.logits, dim=-1)

# Calculate sentiments
all_sentiments = []
for batch_predictions in tqdm(process_batches(df['headline_text'].tolist()), total=len(df) // 32 + 1):
    all_sentiments.extend(batch_predictions[:, 1].tolist())  # Positive sentiment scores

df['sentiment'] = all_sentiments

# Aggregate sentiment by year
sentiment_by_year = df.groupby('year').agg({
    'sentiment': 'sum',
    'headline_text': 'count'
}).rename(columns={'sentiment': 'positive_score', 'headline_text': 'headline_count'})

sentiment_by_year['negative_score'] = sentiment_by_year['headline_count'] - sentiment_by_year['positive_score']
sentiment_by_year['positive_ratio'] = sentiment_by_year['positive_score'] / sentiment_by_year['headline_count']

# Save to CSV
sentiment_by_year.to_csv('sentiment_by_year.csv', index=True)

# Plot 1: Positive and Negative Headline Counts
plt.figure(figsize=(15, 8))
bars = plt.bar(sentiment_by_year.index, sentiment_by_year['positive_score'], label='Positive', color='green')
bars_neg = plt.bar(sentiment_by_year.index, sentiment_by_year['negative_score'],
                   bottom=sentiment_by_year['positive_score'], label='Negative', color='red')

plt.xlabel('Year')
plt.ylabel('Number of Headlines')
plt.title('Sentiment Analysis of Headlines by Year (Using DistilBERT)')
plt.legend(loc='upper left')

# Add percentages to the bars
def add_percentages(bars, bars_neg):
    for bar, bar_neg in zip(bars, bars_neg):
        total = bar.get_height() + bar_neg.get_height()
        if total > 0:  # Avoid division by zero
            positive_percentage = (bar.get_height() / total) * 100
            negative_percentage = (bar_neg.get_height() / total) * 100

            # Add positive percentage to positive bar
            plt.text(
                bar.get_x() + bar.get_width() / 2,
                bar.get_height() / 2,
                f'{positive_percentage:.1f}%',
                ha='center',
                va='center',
                rotation=90,
                color='white',
                fontweight='bold'
            )

            # Add negative percentage to negative bar
            plt.text(
                bar_neg.get_x() + bar_neg.get_width() / 2,
                bar.get_height() + bar_neg.get_height() / 2,
                f'{negative_percentage:.1f}%',
                ha='center',
                va='center',
                rotation=90,
                color='white',
                fontweight='bold'
            )

# Set the figure size explicitly
plt.figure(figsize=(15, 8))

# Plot the bars
bars = plt.bar(sentiment_by_year.index, sentiment_by_year['positive_score'], label='Positive', color='green')
bars_neg = plt.bar(
    sentiment_by_year.index,
    sentiment_by_year['negative_score'],
    bottom=sentiment_by_year['positive_score'],
    label='Negative',
    color='red'
)

# Add percentages
add_percentages(bars, bars_neg)

# Add titles and labels
plt.xlabel('Year')
plt.ylabel('Number of Headlines')
plt.title('Sentiment Analysis of Headlines by Year (Using DistilBERT)')
plt.legend(loc='upper left')

# Adjust layout to prevent overlaps
plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)

# Display the plot
plt.show()

# Plot 2: Positive Sentiment Ratio Over Time
plt.figure(figsize=(15, 8))
sns.lineplot(x=sentiment_by_year.index, y=sentiment_by_year['positive_ratio'], marker='o', color='blue')
plt.title('Positive Sentiment Ratio by Year')
plt.xlabel('Year')
plt.ylabel('Positive Sentiment Ratio')
plt.grid(True)
plt.show()

# Plot 3: Sentiment Score Distribution
plt.figure(figsize=(15, 8))
sns.histplot(df['sentiment'], bins=50, kde=True, color='purple')
plt.title('Distribution of Sentiment Scores')
plt.xlabel('Sentiment Score')
plt.ylabel('Frequency')
plt.show()

# Word Clouds for Positive and Negative Headlines
for year in sentiment_by_year.index:
    yearly_data = df[df['year'] == year]
    positive_texts = " ".join(yearly_data[yearly_data['sentiment'] > 0.5]['headline_text'].tolist())
    negative_texts = " ".join(yearly_data[yearly_data['sentiment'] <= 0.5]['headline_text'].tolist())

    wordcloud_pos = WordCloud(width=800, height=400, background_color='white').generate(positive_texts)
    wordcloud_neg = WordCloud(width=800, height=400, background_color='black').generate(negative_texts)

    # Positive Word Cloud
    plt.figure(figsize=(15, 8))
    plt.imshow(wordcloud_pos, interpolation='bilinear')
    plt.title(f'Positive Headlines Word Cloud ({year})')
    plt.axis('off')
    plt.show()

    # Negative Word Cloud
    plt.figure(figsize=(15, 8))
    plt.imshow(wordcloud_neg, interpolation='bilinear')
    plt.title(f'Negative Headlines Word Cloud ({year})')
    plt.axis('off')
    plt.show()

# Highlight Most Positive and Negative Headlines
top_positive = df.nlargest(10, 'sentiment')[['headline_text', 'sentiment']]
top_negative = df.nsmallest(10, 'sentiment')[['headline_text', 'sentiment']]

print("Top 10 Positive Headlines:")
print(top_positive)

print("\nTop 10 Negative Headlines:")
print(top_negative)

# Create TF-IDF representation
tfidf = TfidfVectorizer(max_features=1000, stop_words='english')
text_features = tfidf.fit_transform(df['headline_text'])

# Apply Non-negative Matrix Factorization
n_topics = 10
nmf = NMF(n_components=n_topics, random_state=42)
topic_features = nmf.fit_transform(text_features)

# Get top words for each topic
def get_top_words(model, feature_names, n_words=10):
    topics = {}
    for topic_idx, topic in enumerate(model.components_):
        top_words = [feature_names[i] for i in topic.argsort()[:-n_words-1:-1]]
        topics[f"Topic {topic_idx+1}"] = top_words
    return topics

topics = get_top_words(nmf, tfidf.get_feature_names_out())

# Assign dominant topic to each headline
df['dominant_topic'] = topic_features.argmax(axis=1)

# Calculate sentiment statistics for each topic
topic_sentiments = df.groupby('dominant_topic').agg({
    'sentiment': ['mean', 'std', 'count']
}).round(3)

# Visualization 1: Topic Sentiment Distribution
plt.figure(figsize=(15, 8))
sns.boxplot(data=df, x='dominant_topic', y='sentiment')
plt.title('Sentiment Distribution Across Topics')
plt.xlabel('Topic Number')
plt.ylabel('Sentiment Score')
plt.grid(True, alpha=0.3)
plt.show()

# Visualization 2: Topic Evolution Over Time
plt.figure(figsize=(15, 8))
topic_evolution = df.groupby([df['publish_date'].dt.year, 'dominant_topic']).size().unstack()
topic_evolution_pct = topic_evolution.div(topic_evolution.sum(axis=1), axis=0)

# Plot stacked area chart
topic_evolution_pct.plot(kind='area', stacked=True)
plt.title('Topic Evolution Over Time')
plt.xlabel('Year')
plt.ylabel('Proportion of Topics')
plt.legend(title='Topic', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Visualization 3: Topic-Sentiment Heatmap
plt.figure(figsize=(12, 8))
topic_sentiment_matrix = pd.pivot_table(
    df,
    values='sentiment',
    index=df['publish_date'].dt.year,
    columns='dominant_topic',
    aggfunc='mean'
)
sns.heatmap(topic_sentiment_matrix,
            cmap='RdYlBu',
            center=0.5,
            annot=True,
            fmt='.2f',
            cbar_kws={'label': 'Average Sentiment'})
plt.title('Topic Sentiment Evolution Over Time')
plt.xlabel('Topic Number')
plt.ylabel('Year')
plt.show()

# Print topic details
print("\nTopic Analysis Summary:")
print("\nTop words for each topic:")
for topic, words in topics.items():
    print(f"\n{topic}: {', '.join(words)}")

print("\nTopic Sentiment Statistics:")
print(topic_sentiments)

# Calculate topic correlations
topic_correlations = defaultdict(list)
for i in range(n_topics):
    for j in range(i+1, n_topics):
        topic_i = topic_features[:, i]
        topic_j = topic_features[:, j]
        correlation = np.corrcoef(topic_i, topic_j)[0, 1]
        if abs(correlation) > 0.3:  # Only show strong correlations
            topic_correlations['Topic Pairs'].append(f"Topic {i+1} - Topic {j+1}")
            topic_correlations['Correlation'].append(correlation)

# Print strong topic correlations
if topic_correlations['Topic Pairs']:
    print("\nStrong Topic Correlations:")
    corr_df = pd.DataFrame(topic_correlations)
    print(corr_df.sort_values('Correlation', ascending=False))

# Additional Analysis: Sentiment Complexity
# Calculate sentiment complexity (variation within topics)
topic_complexity = df.groupby('dominant_topic')['sentiment'].agg(lambda x: np.std(x) * np.log(len(x)))
topic_complexity = topic_complexity.sort_values(ascending=False)

plt.figure(figsize=(12, 6))
topic_complexity.plot(kind='bar')
plt.title('Topic Sentiment Complexity\n(Higher values indicate more nuanced sentiment patterns)')
plt.xlabel('Topic Number')
plt.ylabel('Complexity Score')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()